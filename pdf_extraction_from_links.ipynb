{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF Extraction from Webpages\n",
    "\n",
    "This notebook extracts all PDF document references from a single webpage, including:\n",
    "- Standard HTML links (`<a href=\"...pdf\">`)\n",
    "- Embedded documents (`<iframe>`, `<embed>`, `<object>`)\n",
    "- PDFs referenced in JavaScript code (configuration objects, viewer plugins, etc.)\n",
    "\n",
    "It captures both the PDF URLs and their associated contextual labels where available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing playwright...\n",
      "Installing beautifulsoup4...\n",
      "Installing requests...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] The system cannot find the file specified",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m         subprocess.check_call([sys.executable, \u001b[33m\"\u001b[39m\u001b[33m-m\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpip\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33minstall\u001b[39m\u001b[33m\"\u001b[39m, package, \u001b[33m\"\u001b[39m\u001b[33m-q\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Install Playwright browsers\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mplaywright\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minstall\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mchromium\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcapture_output\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\OrCon\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\subprocess.py:554\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[39m\n\u001b[32m    551\u001b[39m     kwargs[\u001b[33m'\u001b[39m\u001b[33mstdout\u001b[39m\u001b[33m'\u001b[39m] = PIPE\n\u001b[32m    552\u001b[39m     kwargs[\u001b[33m'\u001b[39m\u001b[33mstderr\u001b[39m\u001b[33m'\u001b[39m] = PIPE\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpopenargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[32m    555\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    556\u001b[39m         stdout, stderr = process.communicate(\u001b[38;5;28minput\u001b[39m, timeout=timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\OrCon\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\subprocess.py:1038\u001b[39m, in \u001b[36mPopen.__init__\u001b[39m\u001b[34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[39m\n\u001b[32m   1034\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.text_mode:\n\u001b[32m   1035\u001b[39m             \u001b[38;5;28mself\u001b[39m.stderr = io.TextIOWrapper(\u001b[38;5;28mself\u001b[39m.stderr,\n\u001b[32m   1036\u001b[39m                     encoding=encoding, errors=errors)\n\u001b[32m-> \u001b[39m\u001b[32m1038\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1039\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1040\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1041\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1042\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1043\u001b[39m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1044\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1045\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1046\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1047\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m   1048\u001b[39m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, (\u001b[38;5;28mself\u001b[39m.stdin, \u001b[38;5;28mself\u001b[39m.stdout, \u001b[38;5;28mself\u001b[39m.stderr)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\OrCon\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\subprocess.py:1552\u001b[39m, in \u001b[36mPopen._execute_child\u001b[39m\u001b[34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session, unused_process_group)\u001b[39m\n\u001b[32m   1550\u001b[39m \u001b[38;5;66;03m# Start the process\u001b[39;00m\n\u001b[32m   1551\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1552\u001b[39m     hp, ht, pid, tid = \u001b[43m_winapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCreateProcess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1553\u001b[39m \u001b[43m                             \u001b[49m\u001b[38;5;66;43;03m# no special security\u001b[39;49;00m\n\u001b[32m   1554\u001b[39m \u001b[43m                             \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1555\u001b[39m \u001b[43m                             \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1556\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1557\u001b[39m \u001b[43m                             \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1558\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1559\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1560\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1561\u001b[39m     \u001b[38;5;66;03m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[32m   1562\u001b[39m     \u001b[38;5;66;03m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1565\u001b[39m     \u001b[38;5;66;03m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[32m   1566\u001b[39m     \u001b[38;5;66;03m# ReadFile will hang.\u001b[39;00m\n\u001b[32m   1567\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_pipe_fds(p2cread, p2cwrite,\n\u001b[32m   1568\u001b[39m                          c2pread, c2pwrite,\n\u001b[32m   1569\u001b[39m                          errread, errwrite)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [WinError 2] The system cannot find the file specified"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Install required packages\n",
    "packages = ['playwright', 'beautifulsoup4', 'requests']\n",
    "for package in packages:\n",
    "    try:\n",
    "        __import__(package)\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# Install Playwright browser (correct way)\n",
    "subprocess.check_call([sys.executable, \"-m\", \"playwright\", \"install\", \"chromium\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "from playwright.sync_api import sync_playwright\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDFExtractor:\n",
    "    \"\"\"Extract PDF references from a webpage using multiple detection methods.\"\"\"\n",
    "    \n",
    "    def __init__(self, url: str):\n",
    "        self.url = url\n",
    "        self.base_url = self._get_base_url(url)\n",
    "        self.html_content = None\n",
    "        self.page_object = None\n",
    "        self.pdfs = []\n",
    "    \n",
    "    def _get_base_url(self, url: str) -> str:\n",
    "        \"\"\"Extract base URL for resolving relative paths.\"\"\"\n",
    "        parsed = urlparse(url)\n",
    "        return f\"{parsed.scheme}://{parsed.netloc}\"\n",
    "    \n",
    "    def _normalize_url(self, url: str) -> Optional[str]:\n",
    "        \"\"\"Normalize and validate URLs, handling relative paths and escaped characters.\"\"\"\n",
    "        if not url:\n",
    "            return None\n",
    "        \n",
    "        # Unescape common escape sequences\n",
    "        url = url.replace('\\\\/', '/')\n",
    "        url = url.replace('\\\\\\\\', '\\\\')\n",
    "        \n",
    "        # Convert relative URLs to absolute\n",
    "        if url.startswith('/'):\n",
    "            url = urljoin(self.base_url, url)\n",
    "        elif not url.startswith('http'):\n",
    "            url = urljoin(self.url, url)\n",
    "        \n",
    "        # Ensure it ends with .pdf\n",
    "        if url.lower().endswith('.pdf') or '.pdf' in url.lower():\n",
    "            return url\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def fetch_page(self) -> bool:\n",
    "        \"\"\"Fetch the webpage using Playwright.\"\"\"\n",
    "        try:\n",
    "            with sync_playwright() as p:\n",
    "                browser = p.chromium.launch(headless=True)\n",
    "                page = browser.new_page()\n",
    "                page.goto(self.url, wait_until='networkidle', timeout=30000)\n",
    "                \n",
    "                # Wait for JavaScript to execute\n",
    "                page.wait_for_timeout(2000)\n",
    "                \n",
    "                # Get full HTML content\n",
    "                self.html_content = page.content()\n",
    "                self.page_object = page\n",
    "                \n",
    "                browser.close()\n",
    "                return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching page: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def extract_from_html_links(self) -> List[Dict]:\n",
    "        \"\"\"Extract PDFs from standard HTML <a> tags.\"\"\"\n",
    "        pdfs = []\n",
    "        soup = BeautifulSoup(self.html_content, 'html.parser')\n",
    "        \n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link.get('href', '')\n",
    "            if href.lower().endswith('.pdf') or '.pdf' in href.lower():\n",
    "                normalized_url = self._normalize_url(href)\n",
    "                if normalized_url:\n",
    "                    # Get anchor text\n",
    "                    anchor_text = link.get_text(strip=True)\n",
    "                    \n",
    "                    # Get nearby heading if no anchor text\n",
    "                    if not anchor_text:\n",
    "                        parent = link.find_parent(['div', 'section', 'article'])\n",
    "                        if parent:\n",
    "                            heading = parent.find(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "                            if heading:\n",
    "                                anchor_text = heading.get_text(strip=True)\n",
    "                    \n",
    "                    pdfs.append({\n",
    "                        'url': normalized_url,\n",
    "                        'source': 'HTML Link',\n",
    "                        'anchor_text': anchor_text or 'No text',\n",
    "                        'element': 'a'\n",
    "                    })\n",
    "        \n",
    "        return pdfs\n",
    "    \n",
    "    def extract_from_embedded_documents(self) -> List[Dict]:\n",
    "        \"\"\"Extract PDFs from <iframe>, <embed>, and <object> tags.\"\"\"\n",
    "        pdfs = []\n",
    "        soup = BeautifulSoup(self.html_content, 'html.parser')\n",
    "        \n",
    "        # Extract from iframes\n",
    "        for iframe in soup.find_all('iframe'):\n",
    "            src = iframe.get('src', '')\n",
    "            if src and ('.pdf' in src.lower() or 'pdf' in src.lower()):\n",
    "                normalized_url = self._normalize_url(src)\n",
    "                if normalized_url:\n",
    "                    title = iframe.get('title', '') or iframe.get('name', '')\n",
    "                    pdfs.append({\n",
    "                        'url': normalized_url,\n",
    "                        'source': 'Embedded Document (iframe)',\n",
    "                        'anchor_text': title or 'Embedded PDF',\n",
    "                        'element': 'iframe'\n",
    "                    })\n",
    "        \n",
    "        # Extract from embed tags\n",
    "        for embed in soup.find_all('embed'):\n",
    "            src = embed.get('src', '')\n",
    "            if src and '.pdf' in src.lower():\n",
    "                normalized_url = self._normalize_url(src)\n",
    "                if normalized_url:\n",
    "                    pdfs.append({\n",
    "                        'url': normalized_url,\n",
    "                        'source': 'Embedded Document (embed)',\n",
    "                        'anchor_text': embed.get('title', '') or 'Embedded PDF',\n",
    "                        'element': 'embed'\n",
    "                    })\n",
    "        \n",
    "        # Extract from object tags\n",
    "        for obj in soup.find_all('object'):\n",
    "            data = obj.get('data', '')\n",
    "            if data and '.pdf' in data.lower():\n",
    "                normalized_url = self._normalize_url(data)\n",
    "                if normalized_url:\n",
    "                    pdfs.append({\n",
    "                        'url': normalized_url,\n",
    "                        'source': 'Embedded Document (object)',\n",
    "                        'anchor_text': obj.get('title', '') or 'Embedded PDF',\n",
    "                        'element': 'object'\n",
    "                    })\n",
    "        \n",
    "        return pdfs\n",
    "    \n",
    "    def extract_from_javascript(self) -> List[Dict]:\n",
    "        \"\"\"Extract PDFs from JavaScript code, including configuration objects.\"\"\"\n",
    "        pdfs = []\n",
    "        \n",
    "        # Pattern for common PDF references in JavaScript\n",
    "        patterns = [\n",
    "            r'[\"\\']([^\"\\']*/[^\"\\']*.pdf)[\"\\']',  # String literals with .pdf\n",
    "            r'pdfUrl[\"\\']?\\s*:\\s*[\"\\']([^\"\\']*.pdf)[\"\\']',  # pdfUrl property\n",
    "            r'pdf[\"\\']?\\s*:\\s*[\"\\']([^\"\\']*.pdf)[\"\\']',  # pdf property\n",
    "            r'url[\"\\']?\\s*:\\s*[\"\\']([^\"\\']*.pdf)[\"\\']',  # url property\n",
    "            r'src[\"\\']?\\s*:\\s*[\"\\']([^\"\\']*.pdf)[\"\\']',  # src property\n",
    "            r'file[\"\\']?\\s*:\\s*[\"\\']([^\"\\']*.pdf)[\"\\']',  # file property\n",
    "            r'document[\"\\']?\\s*:\\s*[\"\\']([^\"\\']*.pdf)[\"\\']',  # document property\n",
    "            r'flipbookOptions.*?pdfUrl[\"\\']?\\s*:\\s*[\"\\']([^\"\\']*.pdf)[\"\\']',  # Flipbook specific\n",
    "        ]\n",
    "        \n",
    "        found_urls = set()\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            matches = re.finditer(pattern, self.html_content, re.IGNORECASE | re.DOTALL)\n",
    "            for match in matches:\n",
    "                url = match.group(1)\n",
    "                normalized_url = self._normalize_url(url)\n",
    "                if normalized_url and normalized_url not in found_urls:\n",
    "                    found_urls.add(normalized_url)\n",
    "                    pdfs.append({\n",
    "                        'url': normalized_url,\n",
    "                        'source': 'JavaScript Code',\n",
    "                        'anchor_text': 'JavaScript-embedded PDF',\n",
    "                        'element': 'script'\n",
    "                    })\n",
    "        \n",
    "        # Extract from JSON-like structures in script tags\n",
    "        soup = BeautifulSoup(self.html_content, 'html.parser')\n",
    "        for script in soup.find_all('script', type=None):\n",
    "            if script.string:\n",
    "                # Look for PDF URLs in JSON objects\n",
    "                json_pattern = r'[{,]\\s*[\"\\']?(?:pdf|url|src|file)[\"\\']?\\s*:\\s*[\"\\']([^\"\\']*.pdf)[\"\\']'\n",
    "                matches = re.finditer(json_pattern, script.string, re.IGNORECASE)\n",
    "                for match in matches:\n",
    "                    url = match.group(1)\n",
    "                    normalized_url = self._normalize_url(url)\n",
    "                    if normalized_url and normalized_url not in found_urls:\n",
    "                        found_urls.add(normalized_url)\n",
    "                        # Try to extract context from script id or nearby elements\n",
    "                        script_id = script.get('id', '')\n",
    "                        pdfs.append({\n",
    "                            'url': normalized_url,\n",
    "                            'source': 'JavaScript Configuration',\n",
    "                            'anchor_text': f'Script: {script_id}' if script_id else 'JavaScript Configuration',\n",
    "                            'element': 'script'\n",
    "                        })\n",
    "        \n",
    "        return pdfs\n",
    "    \n",
    "    def extract_all(self) -> List[Dict]:\n",
    "        \"\"\"Extract all PDF references from the webpage.\"\"\"\n",
    "        print(f\"Fetching page: {self.url}\")\n",
    "        if not self.fetch_page():\n",
    "            return []\n",
    "        \n",
    "        print(\"Extracting PDFs from HTML links...\")\n",
    "        self.pdfs.extend(self.extract_from_html_links())\n",
    "        \n",
    "        print(\"Extracting PDFs from embedded documents...\")\n",
    "        self.pdfs.extend(self.extract_from_embedded_documents())\n",
    "        \n",
    "        print(\"Extracting PDFs from JavaScript code...\")\n",
    "        self.pdfs.extend(self.extract_from_javascript())\n",
    "        \n",
    "        # Remove duplicates\n",
    "        seen_urls = set()\n",
    "        unique_pdfs = []\n",
    "        for pdf in self.pdfs:\n",
    "            if pdf['url'] not in seen_urls:\n",
    "                seen_urls.add(pdf['url'])\n",
    "                unique_pdfs.append(pdf)\n",
    "        \n",
    "        self.pdfs = unique_pdfs\n",
    "        return self.pdfs\n",
    "    \n",
    "    def display_results(self):\n",
    "        \"\"\"Display extraction results in a formatted manner.\"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"PDF Extraction Results for: {self.url}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Total PDFs found: {len(self.pdfs)}\\n\")\n",
    "        \n",
    "        if not self.pdfs:\n",
    "            print(\"No PDFs found on this page.\")\n",
    "            return\n",
    "        \n",
    "        for i, pdf in enumerate(self.pdfs, 1):\n",
    "            print(f\"{i}. PDF URL: {pdf['url']}\")\n",
    "            print(f\"   Source: {pdf['source']}\")\n",
    "            print(f\"   Label: {pdf['anchor_text']}\")\n",
    "            print(f\"   Element: {pdf['element']}\")\n",
    "            print()\n",
    "    \n",
    "    def to_json(self) -> str:\n",
    "        \"\"\"Export results as JSON.\"\"\"\n",
    "        return json.dumps(self.pdfs, indent=2)\n",
    "    \n",
    "    def to_csv(self) -> str:\n",
    "        \"\"\"Export results as CSV.\"\"\"\n",
    "        import csv\n",
    "        from io import StringIO\n",
    "        \n",
    "        output = StringIO()\n",
    "        writer = csv.DictWriter(output, fieldnames=['url', 'source', 'anchor_text', 'element'])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(self.pdfs)\n",
    "        return output.getvalue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Extract PDFs from the Siddhartha accreditations page\n",
    "url = \"https://siddhartha.org.in/accreditations/\"\n",
    "\n",
    "extractor = PDFExtractor(url)\n",
    "pdfs = extractor.extract_all()\n",
    "\n",
    "# Display results\n",
    "extractor.display_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export as JSON\n",
    "json_output = extractor.to_json()\n",
    "print(\"JSON Output:\")\n",
    "print(json_output)\n",
    "\n",
    "# Save to file\n",
    "with open('pdf_extraction_results.json', 'w') as f:\n",
    "    f.write(json_output)\n",
    "print(\"\\nResults saved to 'pdf_extraction_results.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export as CSV\n",
    "csv_output = extractor.to_csv()\n",
    "print(\"CSV Output:\")\n",
    "print(csv_output)\n",
    "\n",
    "# Save to file\n",
    "with open('pdf_extraction_results.csv', 'w') as f:\n",
    "    f.write(csv_output)\n",
    "print(\"Results saved to 'pdf_extraction_results.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Usage: Custom URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use this with any URL\n",
    "custom_url = input(\"Enter the URL to extract PDFs from: \")\n",
    "custom_extractor = PDFExtractor(custom_url)\n",
    "custom_pdfs = custom_extractor.extract_all()\n",
    "custom_extractor.display_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Extraction Methods\n",
    "\n",
    "This notebook extracts PDFs using the following methods:\n",
    "\n",
    "1. **HTML Links**: Scans all `<a>` tags with `href` attributes ending in `.pdf`\n",
    "2. **Embedded Documents**: Checks `<iframe>`, `<embed>`, and `<object>` tags for PDF references\n",
    "3. **JavaScript Code**: Uses regex patterns to find PDF URLs in:\n",
    "   - String literals within script tags\n",
    "   - JSON-like configuration objects (e.g., flipbook options)\n",
    "   - Common property names (pdfUrl, pdf, url, src, file, document)\n",
    "\n",
    "### Features:\n",
    "- **URL Normalization**: Converts relative URLs to absolute URLs\n",
    "- **Escape Handling**: Properly handles escaped characters in URLs\n",
    "- **Context Extraction**: Captures anchor text, labels, and nearby headings\n",
    "- **Deduplication**: Removes duplicate PDF URLs\n",
    "- **Multiple Export Formats**: JSON and CSV output options\n",
    "- **Headless Browser**: Uses Playwright for JavaScript execution\n",
    "\n",
    "### Limitations:\n",
    "- Does not perform recursive crawling (single page only)\n",
    "- JavaScript-only PDFs may lack contextual labels\n",
    "- Some obfuscated or dynamically generated URLs may not be detected"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
